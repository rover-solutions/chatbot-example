{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVXlzi4xPYxd"
      },
      "source": [
        "# Treinamento do Chatbot\n",
        "\n",
        "Para começar, é necessário que você tenha uma versão recente do Python e o pip instalados no seu computador, logo mais, vamos instalar as seguintes bibliotecas:\n",
        "\n",
        "- NLTK — É uma das ferramentas mais utilizadas para processamento de linguagem natural, foi desenvolvida em Python e tem uma gama muito grande de recursos, como: classificação, tokenização, stemming, tagging, parsing e raciocínio semântico. Todas essas funções são utilizadas para análise de texto;\n",
        "- Numpy — É uma biblioteca para a linguagem Python com funções para se trabalhar com computação numérica, e que pode realizar operações de álgebra linear de maneira muito eficiente;\n",
        "- Tensorflow — É uma biblioteca de código aberto criada para aprendizado de máquina, computação numérica e muitas outras tarefas;\n",
        "- Keras — Por último, e de extrema importância, usamos o Keras para a estrutura de aprendizado profundo, essa lib poderosíssima é uma das principais APIs de redes neurais de alto nível."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0uvASegPG3S"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weTSGSepQpJY"
      },
      "source": [
        "Agora, vamos criar o arquivo train.py, onde vamos ter o código para ler os dados de linguagem natural e usar a rede neural sequencial keras para criar nosso modelo.\n",
        "Nesse código vamos dividir a explicação em algumas partes, para facilitar o entendimento.\n",
        "Realizamos a importação e configuração inicial das libs que iremos utilizar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ra6dsp5LPiNt",
        "outputId": "d0167281-69eb-4ffe-f70d-0c24fd00a0bd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import nltk\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "if not os.path.exists('models'):\n",
        "    os.makedirs('models')\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9QYcp7-Qs5k"
      },
      "source": [
        "Inicializamos a nossa lista de palavras, classes, documentos e definimos quais palavras serão ignoradas, percorremos a nossa lista de intenções, que foram lidas pelo código e com ajuda do nltk fazemos a tokenização dos patterns e adicionamos na lista de palavras, adicionamos também aos documentos para termos a identificação da tag para cada palavra e adicionamos as tags a nossa lista de classe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqgaPeG5Prso"
      },
      "outputs": [],
      "source": [
        "# inicializaremos nossa lista de palavras, classes, documentos e \n",
        "# definimos quais palavras serão ignoradas\n",
        "words = []\n",
        "documents = []\n",
        "intents = json.loads(open('data/intents.json').read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tKYkmMYPs_v"
      },
      "outputs": [],
      "source": [
        "# adicionamos as tags em nossa lista de classes\n",
        "classes = [i['tag'] for i in intents['intents']]\n",
        "ignore_words = [\"!\", \"@\", \"#\", \"$\", \"%\", \"*\", \"?\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SalZkifPyyY"
      },
      "outputs": [],
      "source": [
        "# percorremos nosso array de objetos\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        # com ajuda no nltk fazemos aqui a tokenizaçao dos patterns \n",
        "        # e adicionamos na lista de palavras\n",
        "        word = nltk.word_tokenize(pattern)\n",
        "        words.extend(word)\n",
        "\n",
        "        # adiciona aos documentos para identificarmos a tag para a mesma\n",
        "        documents.append((word, intent['tag']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnGcL_QsQzQ8"
      },
      "source": [
        "Em seguida, vamos lematizar, ou seja, transformar as palavras em seus significados básicos, com o objetivo de restringir tudo ao nível mais simples possível.\n",
        "Um exemplo de lematização ocorre com verbos, tipo, escrevendo, escreveu e escreve tem o mesmo lema que é escrever.\n",
        "Logo, classificamos nossas listas e estamos prontos para construir o modelo de aprendizado profundo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpZCmC0YP0nK"
      },
      "outputs": [],
      "source": [
        "# lematizamos as palavras ignorando os palavras da lista ignore_words\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg-Dc7u-P222"
      },
      "outputs": [],
      "source": [
        "# classificamos nossas listas\n",
        "words = sorted(list(set(words)))\n",
        "classes = sorted(list(set(classes)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHY1u3czP4Ns"
      },
      "outputs": [],
      "source": [
        "# salvamos as palavras e classes nos arquivos pkl\n",
        "pickle.dump(words, open('models/words.pkl', 'wb'))\n",
        "pickle.dump(classes, open('models/classes.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8K9syk1RpAf"
      },
      "source": [
        "Vamos começar nosso treinamento criando um array vazio de treinamento e criando uma lista de saídas vazias de acordo com o tamanho das nossas classes.\n",
        "Percorremos nossos documentos, inicializamos um array de bag vazio, inserimos no nosso pattern_word a nossa palavra correspondente àquele padrão, lematizamos cada uma delas na tentativa de representar palavras relacionadas, inserimos 1 no bag se a correspondência de palavras for encontrada no pattern atual e utilizamos o output_row como uma chave para a lista, onde a saída será 0 para cada tag e 1 para a tag atual.\n",
        "Após isso embaralhamos nosso conjunto de treinamentos, transformamos em numpy array e definimos uma lista de treinos, sendo x os patterns e y as intenções:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwPIWpv4QE5d"
      },
      "outputs": [],
      "source": [
        "# inicializamos o treinamento\n",
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "for document in documents:\n",
        "    # inicializamos o saco de palavras \n",
        "    bag = []\n",
        "\n",
        "    # listamos as palavras do pattern\n",
        "    pattern_words = document[0]\n",
        "\n",
        "    # lematizamos cada palavra \n",
        "    # na tentativa de representar palavras relacionadas\n",
        "    pattern_words = [lemmatizer.lemmatize( word.lower()) for word in pattern_words]\n",
        "\n",
        "    # criamos nosso conjunto de palavras com 1, \n",
        "    # se a correspondência de palavras for encontrada no padrão     atual\n",
        "    for word in words:\n",
        "        bag.append(1) if word in pattern_words else bag.append(0)\n",
        "\n",
        "    # output_row atuará como uma chave para a lista, \n",
        "    # onde a saida será 0 para cada tag e 1 para a tag atual\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(document[1])] = 1\n",
        "    training.append([bag, output_row])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU0pTQIpQG44",
        "outputId": "47a99ae4-7546-41bd-c7d9-9383ce2f0371"
      },
      "outputs": [],
      "source": [
        "# embaralhamos nosso conjunto de treinamentos e transformamos em numpy array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IcpIDa6QIxZ"
      },
      "outputs": [],
      "source": [
        "# criamos lista de treino sendo x os patterns e y as intenções\n",
        "x = list(training[:, 0])\n",
        "y = list(training[:, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFliPgeqRsHv"
      },
      "source": [
        "Com nossos dados de treinamento prontos, usaremos o modelo de aprendizado profundo keras chamado sequencial, esse modelo sequencial é uma das redes neurais mais simples, um perceptron multicamadas, que em particular tem 3 camadas, com a primeira tendo 128 neurônios, a segunda 64 e a terceira tendo o número de intenções igual o número de neurônios, o objetivo dessa rede é tentar prever qual base escolher de acordo com alguns dados.\n",
        "Esse modelo será treinado com descida gradiente estocástica que é um tópico beeeem complexo, mas que tem muito conteúdo no senhor google e no link disponibilizado.\n",
        "Depois que nosso modelo é treinado será salvo no model.h5 como numpy array e é com esse modelo que vamos criar nossa GUI do chatbot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2zCjm5uQKlj",
        "outputId": "acd28e6c-14fc-45e0-e389-bcca32188c32"
      },
      "outputs": [],
      "source": [
        "# Criamos nosso modelo com 3 camadas. \n",
        "# Primeira camada de 128 neurônios, \n",
        "# segunda camada de 64 neurônios e terceira camada de saída \n",
        "# contém número de neurônios igual ao número de intenções para prever a intenção de saída com softmax\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(y[0]), activation='softmax'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TizRg2IQNyA",
        "outputId": "a18cdec6-0077-4924-e8a5-ec3602294f82"
      },
      "outputs": [],
      "source": [
        "# O modelo é compilado com descida de gradiente estocástica \n",
        "# com gradiente acelerado de Nesterov.\n",
        "# A ideia da otimização do Momentum de Nesterov, ou Nesterov Accelerated Gradient (NAG), \n",
        "# é medir o gradiente da função de custo não na posição local,\n",
        "# mas ligeiramente à frente na direção do momentum. \n",
        "# A única diferença entre a otimização de Momentum é que o gradiente é medido em θ + βm em vez de em θ.\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LfUdGtLQQA-",
        "outputId": "cdc75e2b-f454-4b53-9969-01661fbfca34"
      },
      "outputs": [],
      "source": [
        "# ajustamos e salvamos o modelo\n",
        "m = model.fit(np.array(x), np.array(y), epochs=200, batch_size=5, verbose=1)\n",
        "model.save('models/model.h5', m)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
